{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://data-lake-simple-storage/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import song data and process data for songs and artists tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = input_data + 'song_data/A/A/A/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.680700302124023\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# read song data file\n",
    "start = time.time()\n",
    "df = spark.read.json(song_data)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.0408115386963\n"
     ]
    }
   ],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "start = time.time()\n",
    "\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\", \"artist_id\").parquet(\"s3a://data-lake-simple-storage/\" + \"songs\")\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = df.select(col('artist_id'), \\\n",
    "                          col('artist_name').alias('name'), \\\n",
    "                          col('artist_location').alias('location'), \\\n",
    "                          col('artist_latitude').alias('latitude'), \\\n",
    "                          col('artist_longitude').alias('longitude')).dropDuplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.8410041332245\n"
     ]
    }
   ],
   "source": [
    "# write artists table to parquet files\n",
    "start = time.time()\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data + 'artists')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import log data and process users, time and sonplay tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.325629949569702\n"
     ]
    }
   ],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + 'log_data/2018/11/*.json'\n",
    "\n",
    "# read log data file\n",
    "start = time.time()\n",
    "df = spark.read.json(log_data)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table    \n",
    "users_table = df.select(col('userId').cast('int').alias('user_id'), \\\n",
    "                        col('firstName').alias('first_name'),\\\n",
    "                        col('lastName').alias('last_name'), \\\n",
    "                        col('gender'), \\\n",
    "                        col('level')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "start = time.time()\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data + 'users')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "df = df.withColumn('ts', (F.round(col('ts')/1000)).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.selectExpr('ts AS start_time').dropDuplicates().orderBy('start_time', ascending=False) \\\n",
    "                .withColumn('hour', F.hour('start_time')) \\\n",
    "                .withColumn('day', F.dayofmonth('start_time'))\\\n",
    "                .withColumn('week', F.weekofyear('start_time')) \\\n",
    "                .withColumn('month', F.month('start_time')) \\\n",
    "                .withColumn('year', F.year('start_time')) \\\n",
    "                .withColumn('weekday', F.dayofweek('start_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "start = time.time()\n",
    "time_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_data + 'time')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(output_data + 'songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df.withColumn('songplay_id', F.monotonically_increasing_id()).join(song_df, (song_df.title == df.song))\\\n",
    "                            .select('songplay_id',\\\n",
    "                           col('ts').alias('start_time'),\\\n",
    "                           col('userId').alias('user_id'),\\\n",
    "                           'level',\\\n",
    "                           'song_id',\\\n",
    "                           'artist_id',\\\n",
    "                           col('sessionId').alias('session_id'),\\\n",
    "                           'location',\\\n",
    "                           col('userAgent').alias('user_agent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table = songplays_table \\\n",
    "                .withColumn(\"year\", year(col(\"start_time\"))) \\\n",
    "                .withColumn(\"month\", month(col(\"start_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "start = time.time()\n",
    "songplays_table.write.partitionBy('year', 'month').mode(\"overwrite\").parquet(output_data + 'songplays')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_df = spark.read.parquet(output_data + 'songplays')\n",
    "songplays_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
